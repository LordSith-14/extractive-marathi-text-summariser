{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfd283da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Hardik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "import collections\n",
    "import copy\n",
    "import io\n",
    "import nltk\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "stopwords = set()\n",
    "sentences = []\n",
    "sentences_processing = []\n",
    "sentence_dictionary = collections.defaultdict(dict)\n",
    "stemWords = {}\n",
    "\n",
    "\n",
    "def readStemWords():\n",
    "    '''\n",
    "        Reads the words from the stem words list and transforms the data into usable format\n",
    "     '''\n",
    "#     <आढळ्>[-0000#$$$_N](0){आढळ|1000#$$$Mas,आढळणे|0001#$$$sak}\n",
    "    global stemWords\n",
    "    with io.open(\"word_list_marathi.txt\", encoding='utf-8') as textFile:\n",
    "        index = 0\n",
    "        for line in textFile:\n",
    "            line = line.strip()\n",
    "            if len(line) > 0:\n",
    "                index += 1\n",
    "                wordEndIndex = line.find(\">\")\n",
    "                word = line[2:wordEndIndex]\n",
    "                line = line[wordEndIndex + 1:]                   \n",
    "                baseEndIndex = line.find(\"]\")\n",
    "                base = line[1:baseEndIndex].strip()\n",
    "                line = line[baseEndIndex + 1:]\n",
    "                stem = None\n",
    "                if len(base) >= 0:\n",
    "                    stemEndIndex = base.find('-')                                        \n",
    "                    if stemEndIndex > 0:\n",
    "                        stem = base[:stemEndIndex]\n",
    "\n",
    "#                 valid = line[line.find(\"(\") + 1: line.find(\")\")].strip()\n",
    "#                 if valid == \"0\":\n",
    "#                     continue\n",
    "                line = line[line.find(\"{\") + 1: line.find(\"}\")].strip()\n",
    "                related = []\n",
    "                if len(line) > 0:\n",
    "                    split = line.split(\",\")\n",
    "                    for s in split:\n",
    "                        related.append(s[:s.find(\"|\")])\n",
    "                if stem == None and len(related) > 0:\n",
    "                    stem = related[0]\n",
    "                if stem != None:\n",
    "                    stemWords[word] = {}\n",
    "                    stemWords[word][\"stem\"] = stem\n",
    "                    stemWords[word][\"related\"] = related\n",
    "\n",
    "\n",
    "def tokenize(filename):\n",
    "    '''\n",
    "    Tokenizes the sentences and words\n",
    "    :param filename: path of the file containing the text to be summarized\n",
    "    '''\n",
    "    global sentences, sentences_processing, sentence_dictionary\n",
    "    with io.open(filename, \"r\", encoding=\"utf-8\") as inputFile:\n",
    "        data = inputFile.read()\n",
    "        inputFile.close()\n",
    "    # data = filename.read().decode('utf-8')\n",
    "    # filename.close()\n",
    "    # data=filename\n",
    "    sentences = sent_tokenize(data)\n",
    "    sentences_processing = copy.deepcopy(sentences)\n",
    "    counter = 0\n",
    "    for sentence in sentences_processing:\n",
    "        sentence = sentence[:-1]\n",
    "        sentence = re.sub(',|\\.|-|\\(|\\)', ' ', sentence)\n",
    "        tokens = sentence.strip().split()\n",
    "        actualTokens = removeStopWords(tokens)\n",
    "        stemmedTokens = stemmerMarathi(actualTokens)\n",
    "        sentence_dictionary[counter] = stemmedTokens\n",
    "        counter += 1\n",
    "\n",
    "\n",
    "def readStopWords():\n",
    "    '''\n",
    "    Reads the stopwords from the file\n",
    "    '''\n",
    "    with io.open(\"stopwords.txt\", encoding='utf-8') as textFile:\n",
    "        for line in textFile:\n",
    "            words = line.lower().strip()\n",
    "            stopwords.add(words)\n",
    "        textFile.close()\n",
    "\n",
    "\n",
    "def removeStopWords(wordlist):\n",
    "    '''\n",
    "    Removes the stopwords from the sentences\n",
    "    :param wordlist: list of stopwords\n",
    "    '''\n",
    "    newlist = []\n",
    "    for word in wordlist:\n",
    "        if word not in stopwords:\n",
    "            newlist.append(word)\n",
    "    return newlist\n",
    "\n",
    "\n",
    "def removeCase(word):\n",
    "    '''\n",
    "    :param word: word to be reduced its stem\n",
    "    :return: stem of the word\n",
    "    '''\n",
    "    word_length = len(word) - 1\n",
    "    if word_length > 5:\n",
    "        suffix = \"शया\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "\n",
    "    if word_length > 4:\n",
    "        suffix = \"शे\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "        suffix = \"शी\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "        suffix = \"चा\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "        suffix = \"ची\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "        suffix = \"चे\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "        suffix = \"हून\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "\n",
    "    if word_length > 3:\n",
    "        suffix = \"नो\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "        suffix = \"तो\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "        suffix = \"ने\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "        suffix = \"नी\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "        suffix = \"ही\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "        suffix = \"ते\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "        suffix = \"या\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "        suffix = \"ला\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "        suffix = \"ना\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "        suffix = \"ऊण\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "\n",
    "    if word_length > 2:\n",
    "        suffix = \" े\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "        suffix = \" ी\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "        suffix = \"स\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "        suffix = \"ल\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "        suffix = \" ा\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "        suffix = \"त\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "        suffix = \"म\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "    return word\n",
    "\n",
    "\n",
    "def removeNoGender(word):\n",
    "    global stemWords\n",
    "    orig = word\n",
    "    if word in stemWords:\n",
    "        return stemWords[word][\"stem\"]\n",
    "    word_length = len(word) - 1\n",
    "\n",
    "    if word_length > 5:\n",
    "        suffix = \" ुरडा\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "    if word_length > 4:\n",
    "        suffix = \"ढा\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "    if word_length > 3:\n",
    "        suffix = \"रु\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "        suffix = \"डे\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "        suffix = \"ती\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "        suffix = \" ान\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "        suffix = \" ीण\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "        suffix = \"डा\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "        suffix = \"डी\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "        suffix = \"गा\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "        suffix = \"ला\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "        suffix = \"ळा\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "        suffix = \"या\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "        suffix = \"वा\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "        suffix = \"ये\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "        suffix = \"वे\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "        suffix = \"ती\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "    if word_length > 2:\n",
    "        suffix = \"अ\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "        suffix = \" े\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "        suffix = \"ि \"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "        suffix = \" ु\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "        suffix = \" ौ\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "        suffix = \" ै\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "\n",
    "        suffix = \" ा\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "        suffix = \" ी\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "        suffix = \" ू\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "        suffix = \"त\"\n",
    "        if word.endswith(suffix):\n",
    "            return word[:-len(suffix)]\n",
    "    return word\n",
    "\n",
    "\n",
    "def stemmerMarathi(words):\n",
    "    return [removeNoGender(removeCase(word)) for word in words]\n",
    "\n",
    "filename=\"D:\\Codes\\Final Project\\sample.txt\"\n",
    "\n",
    "def cleanText(filename):\n",
    "    '''\n",
    "        Tokenize, Remove stopwords and reduce the words to their stem\n",
    "    :param filename: path of file to be preprocessed\n",
    "    '''\n",
    "    global sentence_dictionary, sentences\n",
    "    readStopWords()\n",
    "    tokenize(filename)\n",
    "    size = 0\n",
    "    for i in range(0, len(sentence_dictionary)):\n",
    "        size += len(sentence_dictionary[i])\n",
    "    sentence_dictionary = {key: value for key,\n",
    "                           value in sentence_dictionary.items() if len(value) > 0}\n",
    "    return sentence_dictionary, sentences, size\n",
    "\n",
    "\n",
    "readStemWords()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89e1ea26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'dict'>, {})\n"
     ]
    }
   ],
   "source": [
    "print(sentence_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21cfc822",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Hardik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "2024-04-17 10:09:29.950 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\Hardik\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import streamlit as st\n",
    "import collections\n",
    "import os\n",
    "import io\n",
    "import math\n",
    "import operator\n",
    "import sys\n",
    "import networkx as nx\n",
    "from preprocess import cleanText\n",
    "\n",
    "window = 10\n",
    "numberofSentences = 6\n",
    "nodeHash = {}\n",
    "textRank = {}\n",
    "sentenceDictionary = collections.defaultdict(dict)\n",
    "size = 0\n",
    "sentences = []\n",
    "\n",
    "\n",
    "def generatepositionaldistribution():\n",
    "    global nodeHash, sentenceDictionary, sentences, size\n",
    "    positional_dictionary = collections.defaultdict(dict)\n",
    "    count = 0\n",
    "    for i in sentenceDictionary.keys():\n",
    "        for j in range(0, len(sentenceDictionary[i])):\n",
    "            count += 1\n",
    "            position = float(count) / (float(size) + 1.0)\n",
    "            positional_dictionary[i][j] = 1.0 / \\\n",
    "                (math.pi * math.sqrt(position * (1 - position)))\n",
    "            word = sentenceDictionary[i][j]\n",
    "            if word in nodeHash:\n",
    "                if nodeHash[word] < positional_dictionary[i][j]:\n",
    "                    nodeHash[word] = positional_dictionary[i][j]\n",
    "            else:\n",
    "                nodeHash[word] = positional_dictionary[i][j]\n",
    "\n",
    "\n",
    "def textrank():\n",
    "    '''\n",
    "        Generates a graph based ranking model for the tokens\n",
    "    :return: Keyphrases that are most relevant for generating the summary.\n",
    "    '''\n",
    "    global sentenceDictionary, nodeHash, textRank\n",
    "    graph = nx.Graph()\n",
    "    graph.add_nodes_from(nodeHash.keys())\n",
    "    for i in sentenceDictionary.keys():\n",
    "        for j in range(0, len(sentenceDictionary[i])):\n",
    "            current_word = sentenceDictionary[i][j]\n",
    "            next_words = sentenceDictionary[i][j + 1:j + window]\n",
    "            for word in next_words:\n",
    "                graph.add_edge(current_word, word, weight=(\n",
    "                    nodeHash[current_word] + nodeHash[word]) / 2)\n",
    "    textRank = nx.pagerank(graph, weight='weight')\n",
    "    keyphrases = sorted(textRank, key=textRank.get, reverse=True)[:n]\n",
    "    return keyphrases\n",
    "\n",
    "\n",
    "# filepath=\"D:\\Codes\\Final Project\\sample.txt\"\n",
    "def summarize(filepath, keyphrases, numberofSentences):\n",
    "    '''\n",
    "        Generates the summary and writes the summary to the file.\n",
    "    :param filePath: path of file to be used for summarization.\n",
    "    :param keyphrases: Extracted keyphrases\n",
    "    :param numberofSentences: Number of sentences needed as a summary\n",
    "    :output: Writes the summary to the file\n",
    "    '''\n",
    "    global textRank, sentenceDictionary, sentences\n",
    "    sentenceScore = {}\n",
    "    for i in sentenceDictionary.keys():\n",
    "        position = float(i + 1) / (float(len(sentences)) + 1.0)\n",
    "        positionalFeatureWeight = 1.0 / \\\n",
    "            (math.pi * math.sqrt(position * (1.0 - position)))\n",
    "        sumKeyPhrases = 0.0\n",
    "        for keyphrase in keyphrases:\n",
    "            if keyphrase in sentenceDictionary[i]:\n",
    "                sumKeyPhrases += textRank[keyphrase]\n",
    "        sentenceScore[i] = sumKeyPhrases * positionalFeatureWeight\n",
    "    sortedSentenceScores = sorted(sentenceScore.items(\n",
    "    ), key=operator.itemgetter(1), reverse=True)[:numberofSentences]\n",
    "    sortedSentenceScores = sorted(\n",
    "        sortedSentenceScores, key=operator.itemgetter(0), reverse=False)\n",
    "    print(\"\\nSummary: \")\n",
    "    summary = []\n",
    "    arr = []\n",
    "    # for keyphrase in keyphrases:\n",
    "    #     print(keyphrase)\n",
    "    # print(keyphrases)\n",
    "\n",
    "    for i in range(0, len(sortedSentenceScores)):\n",
    "        arr.append(sentences[sortedSentenceScores[i][0]])\n",
    "    s = \"\".join(arr)\n",
    "    # print(s)\n",
    "    return (s)\n",
    "arg1=\"D:\\Codes\\Final Project\\sample.txt\"\n",
    "def process(arg1):\n",
    "    '''\n",
    "    :param arg1: path to the file containing the text to be summarized\n",
    "    :param arg2: Number of sentences to be extracted as summary\n",
    "    :param arg3: size of the window to be used in the co-occurance\n",
    "    '''\n",
    "    arg2 = 5\n",
    "    arg3 = 6\n",
    "    global window, n, numberofSentences, textRank, sentenceDictionary, size, sentences\n",
    "    if arg1 != None and arg2 != None and arg3 != None:\n",
    "        sentenceDictionary, sentences, size = cleanText(arg1)\n",
    "        window = int(arg3)\n",
    "        numberofSentences = int(arg2)\n",
    "        n = int(math.ceil(min(0.1 * size, 7 * math.log(size))))\n",
    "        generatepositionaldistribution()\n",
    "        keyphrases = textrank()\n",
    "        t = summarize(arg1, keyphrases, numberofSentences)\n",
    "        return (t)\n",
    "    else:\n",
    "        print(\"not enough parameters\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "   \n",
    "\n",
    "    # process(sys.argv[1])\n",
    "    st.markdown(\"<h1 style='text-align: center;'>Text Summarization</h1>\",\n",
    "                unsafe_allow_html=True)\n",
    "    uploaded_files = st.file_uploader('Upload text file', type=[\n",
    "                                      'txt'], accept_multiple_files=False)\n",
    "    if uploaded_files is not None:\n",
    "     # To read file as bytes:\n",
    "        #  bytes_data = uploaded_file.getvalue()\n",
    "        #  st.write(bytes_data)\n",
    "        bytes_data = uploaded_files.read().decode('utf-8')\n",
    "        result = process(bytes_data)\n",
    "        st.subheader(\"Input Text\\n\")\n",
    "        st.markdown(\n",
    "            f\"<div style='text-align: justify;'>{bytes_data}</div>\",\n",
    "            unsafe_allow_html=True)\n",
    "        st.subheader(\"Summarized text\\n\")\n",
    "        st.markdown(\n",
    "            f\"<div style='text-align: justify;'>{result}</div>\",\n",
    "            unsafe_allow_html=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eda6e483",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2339740.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[6], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    streamlit run C:\\Users\\Hardik\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "streamlit run C:\\Users\\Hardik\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d33ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
